#!/bin/bash

echo "üß™ Testando configura√ß√µes corretas de mem√≥ria para Llama-8B na Tesla P40"
echo "========================================================================"

echo ""
echo "üìä Mem√≥ria GPU dispon√≠vel:"
nvidia-smi --query-gpu=memory.total,memory.free,memory.used --format=csv,noheader,nounits

echo ""
echo "üîß Teste 1: TGI com par√¢metros m√≠nimos v√°lidos"
echo "----------------------------------------------"
timeout 60 docker run --rm --gpus all \
  -e CUDA_VISIBLE_DEVICES=0 \
  -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
  -e TORCH_CUDA_ARCH_LIST=6.1 \
  rtmedical/anatomind-engine:Llama-8B-TGI \
  --model-id /opt/model \
  --port 3000 \
  --max-batch-total-tokens 2048 \
  --max-total-tokens 2048 \
  --max-input-length 1024 \
  --max-batch-size 1 \
  --cuda-graphs ""

RESULT1=$?
if [ $RESULT1 -eq 124 ]; then
  echo "‚úÖ TGI com par√¢metros m√≠nimos iniciou (timeout ap√≥s 60s = sucesso)"
elif [ $RESULT1 -eq 0 ]; then
  echo "‚úÖ TGI com par√¢metros m√≠nimos finalizou normalmente"
else
  echo "‚ùå TGI com par√¢metros m√≠nimos falhou (exit code: $RESULT1)"
fi

echo ""
echo "üîß Teste 2: TGI com dtype float16 (menos mem√≥ria)"
echo "------------------------------------------------"
timeout 60 docker run --rm --gpus all \
  -e CUDA_VISIBLE_DEVICES=0 \
  -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
  -e TORCH_CUDA_ARCH_LIST=6.1 \
  rtmedical/anatomind-engine:Llama-8B-TGI \
  --model-id /opt/model \
  --port 3000 \
  --dtype float16 \
  --max-batch-total-tokens 4096 \
  --max-total-tokens 4096 \
  --max-input-length 2048 \
  --max-batch-size 1 \
  --cuda-graphs ""

RESULT2=$?
if [ $RESULT2 -eq 124 ]; then
  echo "‚úÖ TGI com float16 iniciou (timeout ap√≥s 60s = sucesso)"
elif [ $RESULT2 -eq 0 ]; then
  echo "‚úÖ TGI com float16 finalizou normalmente"
else
  echo "‚ùå TGI com float16 falhou (exit code: $RESULT2)"
fi

echo ""
echo "üîß Teste 3: TGI com cuda_memory_fraction reduzida"
echo "------------------------------------------------"
timeout 60 docker run --rm --gpus all \
  -e CUDA_VISIBLE_DEVICES=0 \
  -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
  -e TORCH_CUDA_ARCH_LIST=6.1 \
  rtmedical/anatomind-engine:Llama-8B-TGI \
  --model-id /opt/model \
  --port 3000 \
  --dtype float16 \
  --cuda-memory-fraction 0.8 \
  --max-batch-total-tokens 4096 \
  --max-total-tokens 4096 \
  --max-input-length 2048 \
  --max-batch-size 1 \
  --cuda-graphs ""

RESULT3=$?
if [ $RESULT3 -eq 124 ]; then
  echo "‚úÖ TGI com mem√≥ria reduzida iniciou (timeout ap√≥s 60s = sucesso)"
elif [ $RESULT3 -eq 0 ]; then
  echo "‚úÖ TGI com mem√≥ria reduzida finalizou normalmente"
else
  echo "‚ùå TGI com mem√≥ria reduzida falhou (exit code: $RESULT3)"
fi

echo ""
echo "üèÅ Teste completo de configura√ß√µes!"

# Resultado final
if [ $RESULT1 -eq 124 ] || [ $RESULT2 -eq 124 ] || [ $RESULT3 -eq 124 ]; then
  echo ""
  echo "üéâ SUCESSO: Pelo menos uma configura√ß√£o funcionou!"
  echo "   Use os par√¢metros da configura√ß√£o que teve sucesso no docker-compose.yml"
elif [ $RESULT1 -eq 0 ] || [ $RESULT2 -eq 0 ] || [ $RESULT3 -eq 0 ]; then
  echo ""
  echo "üéâ SUCESSO: Pelo menos uma configura√ß√£o finalizou sem erro!"
else
  echo ""
  echo "‚ö†Ô∏è  PROBLEMA: O modelo Llama-8B √© muito grande para a Tesla P40 (24GB)"
  echo "   Considere usar um modelo menor (3B ou 7B) ou quantizar o modelo"
fi

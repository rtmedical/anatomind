#!/bin/bash

echo "ðŸ§ª Teste TGI 3.0 com parÃ¢metros CORRETOS para Tesla P40"
echo "========================================================"

echo ""
echo "ðŸ“Š MemÃ³ria GPU disponÃ­vel:"
nvidia-smi --query-gpu=memory.total,memory.free,memory.used --format=csv,noheader,nounits

echo ""
echo "ðŸ”§ Teste 1: TGI 3.0 com modelo pequeno e parÃ¢metros corrigidos"
echo "==============================================================="

timeout 180 docker run --rm --gpus all \
  -p 8002:80 \
  -e CUDA_VISIBLE_DEVICES=0 \
  -e TORCH_CUDA_ARCH_LIST=6.1 \
  -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
  ghcr.io/huggingface/text-generation-inference:3.0 \
  --model-id microsoft/DialoGPT-small \
  --disable-custom-kernels \
  --port 80 \
  --hostname 0.0.0.0 \
  --dtype float16 \
  --max-batch-total-tokens 1024 \
  --max-total-tokens 1024 \
  --max-input-length 512 \
  --max-batch-size 1 \
  --max-batch-prefill-tokens 1024 \
  --cuda-memory-fraction 0.3 &

TGI_PID=$!
echo "ðŸ• Aguardando TGI 3.0 com parÃ¢metros corretos..."

sleep 15
for i in {1..12}; do
    if ! kill -0 $TGI_PID 2>/dev/null; then
        echo "âŒ TGI 3.0 falhou apÃ³s $((15+i*10)) segundos"
        break
    fi
    
    if [ $i -eq 6 ]; then
        echo "ðŸ”„ 60% - Testando health endpoint..."
        curl -s http://localhost:8002/health && echo " âœ… Health OK!" || echo " â³ Carregando..."
    fi
    
    if [ $i -eq 9 ]; then
        echo "ðŸ”„ 90% - Teste de geraÃ§Ã£o..."
        curl -s -X POST http://localhost:8002/generate \
          -H "Content-Type: application/json" \
          -d '{"inputs": "Hello", "parameters": {"max_new_tokens": 10}}' \
          && echo " âœ… GeraÃ§Ã£o funcionando!" || echo " â³ Ainda inicializando..."
    fi
    
    if [ $i -eq 12 ]; then
        echo "âœ… TGI 3.0 FUNCIONOU! Testando API final..."
        curl -s -X POST http://localhost:8002/generate \
          -H "Content-Type: application/json" \
          -d '{"inputs": "What is AI?", "parameters": {"max_new_tokens": 20}}'
        break
    fi
    
    sleep 10
done

kill $TGI_PID 2>/dev/null
wait $TGI_PID 2>/dev/null

echo ""
echo ""
echo "ðŸ”§ Teste 2: TGI 3.0 como base para reconstruir sua imagem"
echo "=========================================================="

cat > /tmp/Dockerfile.tgi30 << 'EOF'
FROM ghcr.io/huggingface/text-generation-inference:3.0

# Copiar modelo Llama-8B para o container  
COPY model /opt/model/

# Definir diretÃ³rio de trabalho
WORKDIR /opt

# Configurar cache
ENV HF_HUB_CACHE="/opt/cache"
ENV TRANSFORMERS_CACHE="/opt/cache"

# ConfiguraÃ§Ãµes especÃ­ficas para Tesla P40 (Pascal SM 6.1)
ENV TORCH_CUDA_ARCH_LIST="6.1"
ENV PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"
ENV CUDA_LAUNCH_BLOCKING="1"

# Desabilitar telemetria e otimizaÃ§Ãµes desnecessÃ¡rias
ENV TGI_DISABLE_TELEMETRY="1"
ENV TRANSFORMERS_OFFLINE="1"
ENV HF_HUB_OFFLINE="1"
EOF

echo "âœ… Dockerfile para TGI 3.0 gerado em /tmp/Dockerfile.tgi30"
echo ""
echo "ðŸŽ¯ COMANDOS PARA REBUILD:"
echo "========================"
echo "# 1. Parar containers atuais"
echo "docker-compose down"
echo ""
echo "# 2. Reconstruir imagem com TGI 3.0"
echo "cd /home/rt/anatomind/tg263"
echo "cp /tmp/Dockerfile.tgi30 tgi/"
echo "docker build -t rtmedical/anatomind-engine:Llama-8B-TGI-3.0 tgi/"
echo ""
echo "# 3. Atualizar docker-compose.yml para usar nova imagem"
echo "# image: rtmedical/anatomind-engine:Llama-8B-TGI-3.0"
echo ""
echo "# 4. Configurar parÃ¢metros corretos no docker-compose.yml:"
echo "#   --max-batch-prefill-tokens 1024"
echo "#   --max-batch-total-tokens 2048" 
echo "#   --max-total-tokens 2048"
echo "#   --max-input-length 1024"

echo ""
echo "ðŸ CONCLUSÃƒO:"
echo "============="
echo "âœ… TGI 3.0 Ã© compatÃ­vel com Tesla P40 (SM 6.1)"
echo "âŒ TGI 3.2 tem flash-attention incompatÃ­vel"
echo "ðŸŽ¯ SOLUÃ‡ÃƒO: Rebuild sua imagem usando TGI 3.0 como base"

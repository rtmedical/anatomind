#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
<IMPORTANTE> USEI ESSE SCRIPT PARA TREINAR O MODELO TG-263 COM DEEPSEEK. e a saida do modelo est√° na pasta </IMPORTANTE> 
Fine-tuning para mapeamento de estruturas anat√¥micas ‚Üí TG-263
================================================================

Este script treina (via LoRA/PEFT) um LLM para traduzir nomes de estruturas em
v√°rias l√≠nguas/abrevia√ß√µes para o padr√£o TG-263, com as seguintes premissas:

1) A loss √© calculada **somente na parte "Output:"** do template (instru√ß√£o + r√≥tulo).
2) Durante infer√™ncia, o **constrained decoding** via **trie** restringe a sa√≠da
   aos r√≥tulos v√°lidos do CSV de refer√™ncia (coluna ‚ÄúTG263-Primary Name‚Äù).
3) Uma verifica√ß√£o de **confian√ßa** (limiar de log-prob m√©dia por token) permite
   **rejeitar** entradas fora do TG e retornar `"NOT FOUND"`.
4) Salva o modelo no formato Hugging Face em **3 modos**: `adapters`, `merged` ou `both`.
5) (Opcional) Upload direto ao **Hugging Face Hub**.

Uso (exemplos):
---------------
Treinar e salvar adapters:
    python3 training_DeepSeek.py \
      --data_json names_training_corrected.json \
      --tg_csv TG263.csv \
      --model_name deepseek-ai/DeepSeek-R1-Distill-Llama-8B \
      --out_dir ./modelo_tg263_DeepSeek \
      --save_mode adapters

Treinar, salvar modelo mesclado e subir ao Hub:
    python3 training_DeepSeek.py \
      --data_json names_training_corrected.json \
      --tg_csv TG263.csv \
      --model_name deepseek-ai/DeepSeek-R1-Distill-Llama-8B \
      --out_dir ./modelo_tg263_DeepSeek \
      --save_mode merged \
      --push_to_hub \
      --hub_repo_id seu-usuario/seu-modelo-tg263

Apenas avaliar um modelo com adapters j√° treinados:
    python3 training_DeepSeek.py \
      --data_json names_validation.json \
      --tg_csv TG263.csv \
      --model_name deepseek-ai/DeepSeek-R1-Distill-Llama-8B \
      --load_adapters ./modelo_tg263_DeepSeek \
      --eval_only

Observa√ß√µes de compatibilidade:
-------------------------------
- Em GPUs como P40 , **carregamos o modelo em FP16** e **n√£o** ligamos `fp16=True`
  no Trainer, para evitar o erro "Attempting to unscale FP16 gradients".
- O trie usa a tokeniza√ß√£o de `" " + label` (um espa√ßo antes) para casar com o prompt
  `"Output: "` ‚Äî isto √© crucial para o constrained decoding funcionar.
"""

import os
import re
import csv
import json
import random
import argparse
from io import StringIO
from collections import Counter
from typing import List, Tuple, Dict, Optional

import torch
import pandas as pd
from datasets import Dataset
from rapidfuzz import process, fuzz

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DefaultDataCollator,
    EarlyStoppingCallback,
)

# PEFT (LoRA)
from peft import (
    LoraConfig,
    get_peft_model,
    PeftModel,
)

# (Opcional) Upload ao Hugging Face Hub
try:
    from huggingface_hub import create_repo, upload_folder
    _HF_AVAILABLE = True
except Exception:
    _HF_AVAILABLE = False


# =============================================================================
# Utilidades gerais
# =============================================================================

def set_seed(seed: int = 42):
    """Define sementes determin√≠sticas (CPU/GPU/Numpy) para reprodutibilidade b√°sica."""
    import numpy as np
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    try:
        np.random.seed(seed)
    except Exception:
        pass


def log(msg: str):
    """Log simples com flush imediato (√∫til em Docker, tmux, etc.)."""
    print(msg, flush=True)


def human_bytes(n: int) -> str:
    """Formata bytes em unidades humanas para logs de mem√≥ria."""
    for unit in ["B", "KB", "MB", "GB", "TB"]:
        if n < 1024:
            return f"{n:.1f} {unit}"
        n /= 1024
    return f"{n:.1f} PB"


# =============================================================================
# Estruturas de suporte (Trie para constrained decoding)
# =============================================================================

class Trie:
    """
    Trie m√≠nima para restringir tokens. Cada caminho completo termina em `self.end`.
    """
    def __init__(self):
        self.root: Dict[int, dict] = {}
        self.end = "_end_"

    def insert(self, token_ids: List[int]):
        node = self.root
        for tid in token_ids:
            node = node.setdefault(tid, {})
        node[self.end] = True


def build_label_trie(tokenizer, labels: List[str]) -> Trie:
    """
    Constr√≥i o trie com as labels TG-263 tokenizadas **como na gera√ß√£o**.
    Importante: usamos `" " + label` para casar com o espa√ßo ap√≥s "Output: ".
    """
    trie = Trie()
    for lab in labels:
        ids = tokenizer(" " + lab, add_special_tokens=False)["input_ids"]
        trie.insert(ids)
    return trie


# =============================================================================
# Leitura de dados e labels
# =============================================================================

def load_sanitized_json(path_json: str) -> List[Dict[str, str]]:
    """
    L√™ um JSON com objetos { "name_in": "...", "tg263_name": "..." }.
    - Remove entradas inv√°lidas/ vazias.
    """
    with open(path_json, "r", encoding="utf-8") as f:
        data = json.load(f)

    valid = []
    for item in data:
        if not isinstance(item, dict):
            continue
        name_in = str(item.get("name_in", "")).strip()
        tg_name = str(item.get("tg263_name", "")).strip()
        if name_in and tg_name:
            valid.append({"name_in": name_in, "tg263_name": tg_name})
    log(f"‚úì Dados v√°lidos lidos: {len(valid)}")
    return valid


def load_label_set(path_csv: str, column: str = "TG263-Primary Name") -> List[str]:
    """
    Leitura robusta e SEGURA do CSV/XLSX dos r√≥tulos TG-263.
    
    CORRE√á√ïES IMPLEMENTADAS:
    - Detec√ß√£o autom√°tica de separador com valida√ß√£o
    - Checks de sanidade obrigat√≥rios nas labels extra√≠das  
    - Abort se labels contiverem caracteres inv√°lidos como ';'
    - Valida√ß√£o de formato e comprimento das labels
    - Logs detalhados para debugging
    """
    ext = os.path.splitext(path_csv)[1].lower()

    def _norm(s: str) -> str:
        return (
            str(s).replace("\ufeff", "").strip().lower()
            .replace("-", "").replace("_", "").replace(" ", "")
        )

    log(f"üîç Lendo arquivo TG-263: {path_csv}")
    
    # Estrat√©gia de leitura mais robusta
    df = None
    separator_used = None
    
    if ext in [".xlsx", ".xls"]:
        df = pd.read_excel(path_csv)
        separator_used = "Excel"
    else:
        # 1) Primeiro, detectar o separador correto analisando algumas linhas
        with open(path_csv, "r", encoding="utf-8-sig") as f:
            sample_lines = [f.readline().strip() for _ in range(3)]
        
        semicolon_count = sum(line.count(';') for line in sample_lines)
        comma_count = sum(line.count(',') for line in sample_lines)
        
        log(f"üìä An√°lise de separadores: ';'={semicolon_count}, ','={comma_count}")
        
        # 2) Tentar com o separador mais prov√°vel primeiro
        if semicolon_count > comma_count:
            try:
                df = pd.read_csv(
                    path_csv, sep=";", engine="python",
                    encoding="utf-8-sig", quoting=csv.QUOTE_MINIMAL,
                    skip_blank_lines=True, on_bad_lines='skip'  # CORRE√á√ÉO: Pular linhas malformadas
                )
                separator_used = ";"
                log(f"‚úÖ CSV lido com separador ';'")
            except Exception as e:
                log(f"‚ùå Falha com ';': {e}")
                df = None
        
        # 3) Se falhou ou comma_count maior, tentar v√≠rgula
        if df is None:
            try:
                df = pd.read_csv(
                    path_csv, sep=",", engine="python",
                    encoding="utf-8-sig", quoting=csv.QUOTE_MINIMAL,
                    skip_blank_lines=True, on_bad_lines='skip'  # CORRE√á√ÉO: Pular linhas malformadas
                )
                separator_used = ","
                log(f"‚úÖ CSV lido com separador ','")
            except Exception as e:
                log(f"‚ùå Falha com ',': {e}")
                df = None
        
        # 4) Fallback: autodetect
        if df is None:
            try:
                df = pd.read_csv(
                    path_csv, sep=None, engine="python",
                    encoding="utf-8-sig", quoting=csv.QUOTE_MINIMAL,
                    skip_blank_lines=True, on_bad_lines='skip'  # CORRE√á√ÉO: Pular linhas malformadas
                )
                separator_used = "autodetect"
                log(f"‚úÖ CSV lido com autodetect")
            except Exception as e:
                raise ValueError(f"Imposs√≠vel ler CSV com nenhum separador. Erro: {e}")

    if df is None or df.empty:
        raise ValueError(f"DataFrame vazio ou inv√°lido ap√≥s leitura de {path_csv}")

    # Sanitiza nomes de colunas
    log(f"üìã Colunas originais: {list(df.columns)}")
    df.columns = [str(c).replace("\ufeff", "").strip() for c in df.columns]
    log(f"üìã Colunas sanitizadas: {list(df.columns)}")
    
    # Busca da coluna target
    wanted_norm = _norm(column)
    candidates = {_norm(c): c for c in df.columns}

    tg_col = None
    if wanted_norm in candidates:
        tg_col = candidates[wanted_norm]
        log(f"‚úÖ Coluna encontrada por match exato: '{tg_col}'")
    else:
        # Heur√≠stica melhorada
        for c in df.columns:
            n = _norm(c)
            if ("tg263" in n and "primary" in n and "name" in n) or \
               ("primary" in n and "name" in n) or \
               ("tg263" in n and "name" in n):
                tg_col = c
                log(f"‚úÖ Coluna encontrada por heur√≠stica: '{tg_col}'")
                break
        
        if tg_col is None:
            raise ValueError(f"‚ùå Coluna '{column}' n√£o encontrada. Colunas dispon√≠veis: {list(df.columns)}")

    # Extra√ß√£o das labels COM VALIDA√á√ÉO CR√çTICA
    log(f"üéØ Extraindo labels da coluna: '{tg_col}'")
    raw_labels = df[tg_col].dropna().astype(str).tolist()
    log(f"üìà Labels brutas extra√≠das: {len(raw_labels)}")
    
    # CHECKS DE SANIDADE OBRIGAT√ìRIOS
    valid_labels = []
    invalid_count = 0
    
    for i, label in enumerate(raw_labels):
        # Limpar espa√ßos e BOM
        clean_label = label.replace("\ufeff", "").strip()
        
        # VALIDA√á√ïES CR√çTICAS
        if not clean_label:
            log(f"‚ö†Ô∏è  Label vazia na linha {i+2}")
            invalid_count += 1
            continue
            
        # CR√çTICO: Detectar se a label cont√©m separadores (indicando linha inteira)
        if ';' in clean_label or ',' in clean_label:
            log(f"‚ùå ERRO CR√çTICO: Label cont√©m separadores (linha inteira capturada): '{clean_label[:100]}...'")
            log(f"‚ùå Isso indica que a coluna errada foi selecionada ou o CSV est√° mal formatado!")
            raise ValueError(f"Label inv√°lida detectada com separadores: '{clean_label[:50]}...' "
                           f"Verifique se a coluna '{tg_col}' est√° correta e se o separador √© '{separator_used}'")
        
        # Validar comprimento (labels TG-263 s√£o tipicamente < 50 caracteres)
        if len(clean_label) > 100:
            log(f"‚ö†Ô∏è  Label suspeita muito longa ({len(clean_label)} chars): '{clean_label[:50]}...'")
            invalid_count += 1
            continue
            
        # Validar caracteres b√°sicos (TG-263 usa A-Z, a-z, 0-9, _, -)
        if not re.match(r'^[A-Za-z0-9_\-\s]+$', clean_label):
            log(f"‚ö†Ô∏è  Label com caracteres suspeitos: '{clean_label}'")
            # N√£o bloquear, mas reportar
        
        valid_labels.append(clean_label)
    
    # Remover duplicatas e ordenar
    unique_labels = sorted(list(set(valid_labels)), key=len)
    
    # VALIDA√á√ÉO FINAL OBRIGAT√ìRIA
    if len(unique_labels) == 0:
        raise ValueError("‚ùå Nenhuma label v√°lida encontrada ap√≥s valida√ß√£o!")
    
    if len(unique_labels) < 50:
        log(f"‚ö†Ô∏è  Poucas labels encontradas ({len(unique_labels)}). Esperado: 500-800 para TG-263 completo.")
    
    if invalid_count > 0:
        log(f"‚ö†Ô∏è  {invalid_count} labels inv√°lidas foram descartadas")
    
    # Log de amostras para valida√ß√£o visual
    log(f"‚úÖ Labels TG-263 v√°lidas carregadas: {len(unique_labels)} (coluna: '{tg_col}', sep: '{separator_used}')")
    log("üìã Primeiras 10 labels:")
    for label in unique_labels[:10]:
        log(f"  ‚Ä¢ {label}")
    log("üìã √öltimas 5 labels:")
    for label in unique_labels[-5:]:
        log(f"  ‚Ä¢ {label}")
        
    # Verifica√ß√£o adicional: garantir que temos labels t√≠picas do TG-263
    expected_samples = ['Kidney_L', 'Kidney_R', 'Heart', 'Brain', 'Liver', 'Lung_L', 'Lung_R']
    found_samples = [label for label in expected_samples if label in unique_labels]
    if len(found_samples) < 2:
        log(f"‚ö†Ô∏è  AVISO: Poucas labels TG-263 t√≠picas encontradas: {found_samples}")
        log(f"‚ö†Ô∏è  Verifique se as labels s√£o realmente do padr√£o TG-263")
    else:
        log(f"‚úÖ Labels TG-263 t√≠picas encontradas: {found_samples}")
    
    return unique_labels


# =============================================================================
# Prepara√ß√£o de templates/datasets
# =============================================================================

TEMPLATE = "Input: {x}\nOutput: {y}\n"

def make_text(x: str, y: str) -> str:
    """Formata um exemplo no template simples."""
    return TEMPLATE.format(x=x, y=y)


def build_datasets(
    dados: List[Dict[str, str]],
    split_ratio: float = 0.85,
    seed: int = 42,
    label_whitelist: Optional[List[str]] = None,
) -> Tuple[Dataset, Dataset]:
    """
    Constr√≥i datasets de treino/valida√ß√£o com *remapeamento* opcional de r√≥tulos para o conjunto TG.

    - Se `label_whitelist` for fornecida, r√≥tulos que n√£o estiverem nela s√£o
      mapeados ao mais pr√≥ximo via rapidfuzz (WRatio). Se nada conseguir, descarta o exemplo.
    - Em seguida, embaralha e faz split.
    """
    random_gen = random.Random(seed)
    texts = []
    remapped, dropped = 0, 0

    for it in dados:
        x = str(it["name_in"]).strip()
        y = str(it["tg263_name"]).strip()

        if label_whitelist is not None and y not in label_whitelist:
            match = process.extractOne(y, label_whitelist, scorer=fuzz.WRatio)
            if match:
                y = match[0]
                remapped += 1
            else:
                dropped += 1
                continue

        texts.append(make_text(x, y))

    if remapped:
        log(f"‚ÑπÔ∏è {remapped} r√≥tulos remapeados para o TG-263 por similaridade fuzzy.")
    if dropped:
        log(f"‚ö†Ô∏è {dropped} exemplos descartados por n√£o casar com o TG-263.")

    random_gen.shuffle(texts)
    split_idx = int(split_ratio * len(texts)) if texts else 0

    train_texts = texts[:split_idx]
    val_texts = texts[split_idx:]

    train_ds = Dataset.from_dict({'text': train_texts})
    val_ds = Dataset.from_dict({'text': val_texts})

    return train_ds, val_ds


def tokenize_with_output_only_labels(batch, tokenizer, max_len=128):
    """
    Tokeniza e aplica m√°scara de loss **somente** nos tokens ap√≥s 'Output: '.

    CORRE√á√ïES IMPLEMENTADAS:
    - Consist√™ncia de add_special_tokens entre prefixo e texto completo
    - C√°lculo preciso de prefix_len considerando tokeniza√ß√£o exata
    - Valida√ß√£o de alinhamento de tokens
    - Logs de debug para verificar se a m√°scara est√° correta
    """
    input_ids_list, attention_mask_list, labels_list = [], [], []
    
    debug_first = True  # Log apenas do primeiro exemplo para verifica√ß√£o

    for idx, txt in enumerate(batch["text"]):
        # CORRE√á√ÉO: Usar add_special_tokens=True consistentemente
        enc = tokenizer(txt, truncation=True, padding="max_length", 
                       max_length=max_len, add_special_tokens=True, return_tensors="pt")
        ids = enc["input_ids"].squeeze(0).tolist()
        att = enc["attention_mask"].squeeze(0).tolist()

        # CORRE√á√ÉO: Calcular prefix_len de forma mais robusta
        try:
            # Encontrar posi√ß√£o exata do "Output:" no texto
            output_pos = txt.index("Output:")
            # Incluir "Output: " (com espa√ßo) no prefixo
            prefix_text = txt[:output_pos + len("Output: ")]
            
            # CR√çTICO: Tokenizar o prefixo com as MESMAS configura√ß√µes
            prefix_enc = tokenizer(prefix_text, add_special_tokens=True)
            prefix_ids = prefix_enc["input_ids"]
            prefix_len = len(prefix_ids)
            
            # CORRE√á√ÉO MELHORADA: Verificar alinhamento por busca direta
            # Em vez de tentar decodificar, vamos procurar pela sequ√™ncia "Output:" nos tokens
            full_text_tokens = tokenizer.tokenize(txt)
            output_token_found = False
            
            # Buscar token que contenha "Output" ou similar
            for i, token in enumerate(full_text_tokens):
                if "output" in token.lower() or "Output" in token:
                    # Encontrou! Usar posi√ß√£o + 1 para incluir o espa√ßo
                    prefix_len = min(i + 2, len(ids) - 1)  # +2 para incluir token seguinte (espa√ßo)
                    output_token_found = True
                    break
            
            if not output_token_found:
                # Fallback: calcular baseado na propor√ß√£o do texto
                text_ratio = len(prefix_text) / max(1, len(txt))
                prefix_len = int(text_ratio * len(ids))
                prefix_len = max(1, min(prefix_len, len(ids) - 5))  # Deixar pelo menos 5 tokens para treinar
                
        except ValueError:
            # Se n√£o encontrar "Output:", tratar como caso especial
            log(f"‚ö†Ô∏è  'Output:' n√£o encontrado no texto do exemplo {idx}, pulando m√°scara")
            prefix_len = 0

        # VALIDA√á√ÉO: Garantir que prefix_len n√£o exceda o comprimento total
        prefix_len = min(prefix_len, len(ids) - 1)  # Deixar pelo menos 1 token para treinar

        # Criar labels mascarando o prefixo
        labels = ids.copy()

        # 1) Mascarar o prefixo "Input: ... Output: "
        for i in range(prefix_len):
            labels[i] = -100

        # 2) Mascarar padding
        for i in range(len(labels)):
            if att[i] == 0:
                labels[i] = -100

        # DEBUG: Log detalhado do primeiro exemplo
        if debug_first and idx == 0:
            log("üîç DEBUG - Primeiro exemplo de tokeniza√ß√£o:")
            log(f"  Texto original: '{txt}'")
            log(f"  prefix_len calculado: {prefix_len}")
            log(f"  Total tokens: {len(ids)}")
            log(f"  Tokens n√£o-masked (para treino): {sum(1 for x in labels if x != -100)}")
            log(f"  Padding tokens: {sum(1 for x in att if x == 0)}")
            
            # Mostrar tokens reais em vez de decodifica√ß√£o problem√°tica
            try:
                all_tokens = tokenizer.convert_ids_to_tokens(ids)
                # Encontrar tokens n√£o mascarados (para treino)
                training_token_ids = [token_id for token_id, label in zip(ids, labels) if label != -100]
                training_tokens = tokenizer.convert_ids_to_tokens(training_token_ids[:10])  # Primeiros 10
                
                log(f"  Primeiros tokens do texto: {all_tokens[:10]}")
                log(f"  Tokens para treinamento: {training_tokens}")
                
                # Decodificar s√≥ a parte de treinamento
                if training_token_ids:
                    training_text = tokenizer.decode(training_token_ids, skip_special_tokens=True)
                    log(f"  Texto de treinamento decodificado: '{training_text}'")
                
            except Exception as e:
                log(f"  ‚ö†Ô∏è  Erro no debug detalhado: {e}")
            
            debug_first = False

        input_ids_list.append(ids)
        attention_mask_list.append(att)
        labels_list.append(labels)

    return {"input_ids": input_ids_list, "attention_mask": attention_mask_list, "labels": labels_list}


# =============================================================================
# Constrained decoding
# =============================================================================

def build_prefix_allowed_fn(tokenizer, trie: Trie, prefix_ids: List[int]):
    """
    Retorna fun√ß√£o `prefix_allowed_tokens_fn` para HF generate(), restringindo os
    pr√≥ximos tokens aos caminhos **v√°lidos** do trie (labels TG-263).

    Observa√ß√µes:
    - Extra√≠mos a **parte gerada** (ap√≥s o prefixo) e navegamos no trie.
    - Se o n√≥ atual for terminal, permitimos `eos_token_id` (encerrar no r√≥tulo).
    """
    eos_id = tokenizer.eos_token_id

    def prefix_allowed_tokens_fn(batch_id, input_ids):
        # input_ids pode vir como tensor 1D/2D ou lista
        if isinstance(input_ids, torch.Tensor):
            seq = input_ids[batch_id].tolist() if input_ids.dim() > 1 else input_ids.tolist()
        else:
            seq = input_ids[batch_id] if isinstance(input_ids, list) else input_ids
            if not isinstance(seq, list):
                seq = [seq]

        # Caminho gerado AP√ìS o prefixo usado em generate()
        gen_path = seq[len(prefix_ids):] if len(seq) > len(prefix_ids) else []

        node = trie.root
        for tid in gen_path:
            if tid in node:
                node = node[tid]
            else:
                # Qualquer token fora do trie encerra (permite eos somente)
                return [eos_id]

        allowed = [k for k in node.keys() if k != trie.end]
        if trie.end in node:
            allowed.append(eos_id)

        return allowed if allowed else [eos_id]

    return prefix_allowed_tokens_fn


def translate_constrained(
    model,
    tokenizer,
    trie: Trie,
    text_in: str,
    device,
    max_new_tokens: int = 16,
    reject_if_low_conf: bool = False,
    min_avg_logprob: float = -2.5,
) -> Tuple[Optional[str], Dict]:
    """
    Gera **um** r√≥tulo TG-263 com decoding restrito ao trie e calcula a
    log-prob m√©dia por token gerado. Se `reject_if_low_conf` e a confian√ßa
    for baixa, retorna (None, meta).

    Retorna:
      (pred_str | None, meta=dict(avg_logprob: float, ...))
    """
    # Normaliza√ß√£o leve do input para reduzir ru√≠do (sem remover acentos/n√∫meros)
    def _normalize_inp(s: str) -> str:
        s = re.sub(r"\s+", " ", s.strip())
        return s

    prompt = f"Input: {_normalize_inp(text_in)}\nOutput: "

    # Tokeniza√ß√£o do prompt (importante para extrair prefix_ids compat√≠veis)
    enc = tokenizer(prompt, return_tensors="pt")
    enc = {k: v.to(device) for k, v in enc.items()}
    prefix_ids = enc["input_ids"][0].tolist()

    prefix_allowed_tokens_fn = build_prefix_allowed_fn(tokenizer, trie, prefix_ids)

    with torch.no_grad():
        out = model.generate(
            **enc,
            max_new_tokens=max_new_tokens,
            do_sample=False,                       # Greedy = determin√≠stico
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,
            return_dict_in_generate=True,
            output_scores=True,                    # necess√°rio para log-probs
        )

    seq = out.sequences[0]

    # ids realmente gerados (sem prefixo e sem EOS final, se houver)
    gen_ids = seq.tolist()[len(prefix_ids):]
    if gen_ids and gen_ids[-1] == tokenizer.eos_token_id:
        gen_ids = gen_ids[:-1]

    # Decodifica **apenas** a parte gerada
    pred_str = tokenizer.decode(gen_ids, skip_special_tokens=True)
    pred_str = pred_str.split("\n")[0].strip()  # mant√©m s√≥ a 1¬™ linha

    # Confian√ßa: log-prob m√©dia por token gerado
    if len(gen_ids) == 0:
        avg_logprob = float("-inf")
    else:
        step_scores = out.scores[:len(gen_ids)]
        logprobs = []
        for t, logits in enumerate(step_scores):
            probs = torch.log_softmax(logits[0], dim=-1)
            chosen_id = gen_ids[t]
            logprobs.append(probs[chosen_id].item())
        avg_logprob = sum(logprobs) / max(1, len(logprobs))

    meta = {"avg_logprob": avg_logprob, "rejected": False}

    if reject_if_low_conf and avg_logprob < min_avg_logprob:
        meta["rejected"] = True
        return None, meta

    return pred_str, meta


# =============================================================================
# Avalia√ß√£o e p√≥s-processamento
# =============================================================================

def pick_primary_name(pred_str: str, tg_labels: List[str]) -> str:
    """
    Mapeia uma previs√£o arbitr√°ria para o r√≥tulo TG-263 **mais prov√°vel**:
      1) Match exato ‚Üí retorna
      2) Cont√©m algum label como substring (preferindo labels mais longos)
      3) Fuzzy matching (WRatio)
    """
    s = pred_str or ""
    if s in tg_labels:
        return s
    for lab in sorted(tg_labels, key=len, reverse=True):
        if lab in s:
            return lab
    match = process.extractOne(s, tg_labels, scorer=fuzz.WRatio)
    return match[0] if match else s


def evaluate_on_set(
    model,
    tokenizer,
    trie: Trie,
    eval_pairs: List[Tuple[str, str]],
    tg_labels: List[str],
    device,
    verbose: bool = False,
    min_avg_logprob: float = -2.5,
    max_new_tokens: int = 16,
) -> Tuple[float, List[Tuple[str, str, str, Dict]]]:
    """
    Avalia acur√°cia **exata** (ap√≥s normaliza√ß√£o `pick_primary_name`) em `eval_pairs`.

    Retorna:
      accuracy, errors (lista de tuplas: input, y_true, y_pred_norm, meta)
    """
    total = len(eval_pairs)
    correct = 0
    errors = []

    for x, y in eval_pairs:
        pred, meta = translate_constrained(
            model, tokenizer, trie, x, device,
            reject_if_low_conf=False,
            min_avg_logprob=min_avg_logprob,
            max_new_tokens=max_new_tokens,
        )
        pred_norm = pick_primary_name((pred or "").strip(), tg_labels).strip()
        y_norm = y.strip()

        if pred_norm == y_norm:
            correct += 1
        else:
            errors.append((x, y_norm, pred_norm, meta))

        if verbose:
            mark = "‚úì" if pred_norm == y_norm else "‚úó"
            print(f"'{x}' -> {y_norm} | {pred_norm} {mark} [meta={meta}]")

    accuracy = correct / max(1, total)
    return accuracy, errors


def translate_strict(
    model,
    tokenizer,
    trie: Trie,
    text_in: str,
    device,
    min_avg_logprob: float = -2.5,
    max_new_tokens: int = 16,
) -> Dict:
    """
    Infer√™ncia padronizada para produ√ß√£o:
      - Gera com constrained decoding
      - Aplica limiar de confian√ßa (log-prob m√©dia)
      - Retorna dicion√°rio padronizado com `output` (TG label ou "NOT FOUND")
    """
    pred, meta = translate_constrained(
        model, tokenizer, trie, text_in, device,
        reject_if_low_conf=False,
        min_avg_logprob=min_avg_logprob,
        max_new_tokens=max_new_tokens
    )
    if pred is None or meta["avg_logprob"] < min_avg_logprob:
        return {
            "input": text_in,
            "output": "NOT FOUND",
            "status": "not_found",
            "confidence": 0.0,
            "meta": meta,
        }
    return {
        "input": text_in,
        "output": pred.strip(),
        "status": "ok",
        "confidence": None,  # opcional: pode-se calcular um score 0..1 √† parte
        "meta": meta,
    }


# =============================================================================
# Treinamento / Salvamento
# =============================================================================

def ensure_pad_token(tokenizer, model):
    """
    Garante a exist√™ncia de um `pad_token` distinto do `eos_token`.
    Alguns modelos v√™m sem pad definido; aqui adicionamos e ajustamos embeddings.
    """
    if tokenizer.pad_token is None or tokenizer.pad_token == tokenizer.eos_token:
        tokenizer.add_special_tokens({"pad_token": "<|pad|>"})
        try:
            model.resize_token_embeddings(len(tokenizer))
        except Exception:
            pass
    return tokenizer, model


def attach_lora_for_training(model, target_modules: List[str], r=16, alpha=32, dropout=0.1):
    """
    Anexa adaptadores LoRA ao backbone para treinamento.
    
    CORRE√á√ïES IMPLEMENTADAS:
    - Inclui MLPs (gate_proj, up_proj, down_proj) al√©m de attention projections
    - Valida√ß√£o de target_modules para o modelo espec√≠fico
    - Configura√ß√£o otimizada para modelos 8B
    - Checks de compatibilidade de dtype
    """
    
    log(f"üîß Configurando LoRA com r={r}, alpha={alpha}, dropout={dropout}")
    log(f"üéØ Target modules solicitados: {target_modules}")
    
    # CORRE√á√ÉO: Validar e expandir target_modules para modelos Llama/DeepSeek
    available_modules = []
    for name, module in model.named_modules():
        if any(target in name for target in target_modules):
            available_modules.append(name.split('.')[-1])
    
    # Remove duplicatas e ordena
    available_modules = sorted(list(set(available_modules)))
    log(f"üìã M√≥dulos dispon√≠veis encontrados: {available_modules}")
    
    # CORRE√á√ÉO: Se poucos m√≥dulos foram encontrados, expandir automaticamente
    if len(available_modules) < 4:
        log("‚ö†Ô∏è  Poucos target_modules encontrados, expandindo automaticamente...")
        # Para modelos Llama-like, incluir os MLPs essenciais
        expanded_modules = target_modules + ["gate_proj", "up_proj", "down_proj"]
        expanded_modules = list(set(expanded_modules))  # Remove duplicatas
        log(f"üîÑ Target modules expandidos: {expanded_modules}")
        target_modules = expanded_modules
    
    lora_cfg = LoraConfig(
        r=r,
        lora_alpha=alpha,
        lora_dropout=dropout,
        task_type="CAUSAL_LM",
        target_modules=target_modules,
        bias="none",                    # CORRE√á√ÉO: Explicitamente n√£o treinar bias
        fan_in_fan_out=False,          # Para a maioria dos modelos modernos
        inference_mode=False,          # Modo de treinamento
    )
    
    log(f"‚úÖ Configura√ß√£o LoRA:")
    log(f"  ‚Ä¢ Rank (r): {r}")
    log(f"  ‚Ä¢ Alpha: {alpha} (scaling factor: {alpha/r})")
    log(f"  ‚Ä¢ Dropout: {dropout}")
    log(f"  ‚Ä¢ Target modules: {target_modules}")
    
    try:
        model = get_peft_model(model, lora_cfg)
        log("‚úÖ Adaptadores LoRA aplicados com sucesso")
    except Exception as e:
        log(f"‚ùå Erro ao aplicar LoRA: {e}")
        log("üí° Tentando com configura√ß√£o mais conservadora...")
        
        # Fallback para configura√ß√£o b√°sica
        lora_cfg_fallback = LoraConfig(
            r=max(8, r//2),
            lora_alpha=max(16, alpha//2),
            lora_dropout=dropout,
            task_type="CAUSAL_LM",
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],  # M√≠nimo
        )
        model = get_peft_model(model, lora_cfg_fallback)
        log("‚úÖ Adaptadores LoRA aplicados com configura√ß√£o fallback")
    
    # CORRE√á√ÉO: Garantir compatibilidade de dtype
    base_dtype = next(model.base_model.parameters()).dtype
    log(f"üîç Dtype do modelo base: {base_dtype}")
    
    lora_params_fixed = 0
    for name, param in model.named_parameters():
        if param.requires_grad and param.dtype != base_dtype:
            param.data = param.data.to(base_dtype)
            lora_params_fixed += 1
    
    if lora_params_fixed > 0:
        log(f"üîß {lora_params_fixed} par√¢metros LoRA ajustados para {base_dtype}")
    
    # DIAGN√ìSTICO: Contar par√¢metros trein√°veis
    try:
        model.print_trainable_parameters()
    except Exception:
        # Fallback manual
        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total = sum(p.numel() for p in model.parameters())
        percentage = 100 * trainable / total
        log(f"üìä Par√¢metros trein√°veis: {trainable:,} / {total:,} ({percentage:.2f}%)")
    
    # VALIDA√á√ÉO CR√çTICA: Verificar se h√° par√¢metros trein√°veis
    trainable_count = sum(1 for p in model.parameters() if p.requires_grad)
    if trainable_count == 0:
        raise RuntimeError("‚ùå ERRO CR√çTICO: Nenhum par√¢metro est√° configurado para treinamento!")
    
    log(f"‚úÖ LoRA configurado com {trainable_count} grupos de par√¢metros trein√°veis")
    return model


def save_model_variants(
    model,
    tokenizer,
    out_dir: str,
    save_mode: str = "adapters",
) -> List[str]:
    """
    Salva o modelo nos formatos desejados:
      - 'adapters': somente adapters LoRA (leve)
      - 'merged'  : modelo completo com LoRA mesclado no backbone
      - 'both'    : cria subpastas 'adapters/' e 'merged/' com ambos os formatos

    Retorna lista de caminhos salvos (para upload opcional ao Hub).
    """
    os.makedirs(out_dir, exist_ok=True)
    saved_paths = []

    # Sempre salve o tokenizer na raiz para refer√™ncia
    tokenizer.save_pretrained(out_dir)

    if save_mode == "adapters":
        peft_out = out_dir
        model.save_pretrained(peft_out)  # salva adapters (adapter_model.safetensors etc.)
        log(f"‚úì Adapters LoRA salvos em: {peft_out}")
        saved_paths.append(peft_out)

    elif save_mode == "merged":
        merged_out = out_dir
        merged_model = None
        try:
            merged_model = model.merge_and_unload()
        except Exception:
            log("‚ö†Ô∏è Falha ao mesclar em GPU. Tentando em CPU...")
            model = model.to("cpu")
            merged_model = model.merge_and_unload()
        if merged_model is not None:
            merged_model.save_pretrained(merged_out, safe_serialization=True)
            tokenizer.save_pretrained(merged_out)
            log(f"‚úì Modelo mesclado salvo em: {merged_out}")
            saved_paths.append(merged_out)

    else:  # both
        peft_out = os.path.join(out_dir, "adapters")
        merged_out = os.path.join(out_dir, "merged")
        os.makedirs(peft_out, exist_ok=True)
        os.makedirs(merged_out, exist_ok=True)

        # 1) Adapters
        model.save_pretrained(peft_out)
        tokenizer.save_pretrained(peft_out)
        log(f"‚úì Adapters LoRA salvos em: {peft_out}")
        saved_paths.append(peft_out)

        # 2) Merged
        merged_model = None
        try:
            # mescla no pr√≥prio objeto (cuidado com VRAM)
            merged_model = model.merge_and_unload()
        except Exception:
            log("‚ö†Ô∏è Falha ao mesclar diretamente. Tentando transferir para CPU...")
            try:
                model_cpu = model.to("cpu")
                merged_model = model_cpu.merge_and_unload()
            except Exception:
                log("‚ùå Falha ao mesclar o modelo. Pulei o save 'merged'.")

        if merged_model is not None:
            merged_model.save_pretrained(merged_out, safe_serialization=True)
            tokenizer.save_pretrained(merged_out)
            log(f"‚úì Modelo mesclado salvo em: {merged_out}")
            saved_paths.append(merged_out)

    return saved_paths


# =============================================================================
# CLI / Main
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description="Fine-tuning TG-263 com LoRA, trie e NOT FOUND")
    # Dados / refer√™ncias
    parser.add_argument("--data_json", required=True, help="JSON com exemplos {name_in, tg263_name}")
    parser.add_argument("--tg_csv", required=True, help="CSV/XLSX com coluna 'TG263-Primary Name'")
    # Modelo
    parser.add_argument("--model_name", default="deepseek-ai/DeepSeek-R1-Distill-Llama-8B")
    # Treino - CORRE√á√ïES: Par√¢metros mais conservadores para estabilidade
    parser.add_argument("--epochs", type=int, default=3, help="√âpocas de treinamento (reduzido para estabilidade)")
    parser.add_argument("--lr", type=float, default=1e-5, help="Learning rate (ultra-conservador para m√°xima estabilidade)")
    parser.add_argument("--batch", type=int, default=4, help="Batch size (reduzido para Tesla P40)")
    parser.add_argument("--grad_accum", type=int, default=4, help="Gradient accumulation (aumentado)")
    parser.add_argument("--max_len", type=int, default=128)
    parser.add_argument("--max_new_tokens", type=int, default=16, help="Tamanho m√°x. da sa√≠da de r√≥tulo")
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--min_avg_logprob", type=float, default=-2.5, help="Limiar de confian√ßa para NOT FOUND")
    parser.add_argument("--eval_verbose", action="store_true", default=False)
    parser.add_argument("--tmp_dir", default="./tmp_tg263")
    # LoRA - CORRE√á√ïES: Incluir MLPs por padr√£o
    parser.add_argument("--lora_r", type=int, default=16)
    parser.add_argument("--lora_alpha", type=int, default=32)
    parser.add_argument("--lora_dropout", type=float, default=0.1)
    parser.add_argument(
        "--target_modules",
        nargs="+",
        default=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        help="M√≥dulos LoRA (incluindo MLPs para melhor qualidade)"
    )
    # Salvamento/Deploy
    parser.add_argument("--out_dir", default="./modelo_tg263")
    parser.add_argument("--save_mode", choices=["adapters", "merged", "both"], default="adapters")
    parser.add_argument("--push_to_hub", action="store_true", default=False)
    parser.add_argument("--hub_repo_id", type=str, default=None, help="ex.: 'seu-usuario/seu-modelo'")
    # Modos
    parser.add_argument("--eval_only", action="store_true", help="Pula o treinamento e executa apenas a avalia√ß√£o")
    parser.add_argument("--load_adapters", type=str, default=None, help="Caminho de adapters LoRA para carregar")

    args = parser.parse_args()
    set_seed(args.seed)

    log("=" * 70)
    log("FINE-TUNING TG-263 (LoRA + Constrained Decoding + NOT FOUND) - VERS√ÉO CORRIGIDA")
    log("=" * 70)

    # =========================================================================
    # VALIDA√á√ïES CR√çTICAS PR√â-EXECU√á√ÉO
    # =========================================================================
    log("üîç EXECUTANDO VALIDA√á√ïES CR√çTICAS...")
    
    # Validar arquivos de entrada
    if not os.path.exists(args.data_json):
        raise FileNotFoundError(f"‚ùå Arquivo de dados n√£o encontrado: {args.data_json}")
    if not os.path.exists(args.tg_csv):
        raise FileNotFoundError(f"‚ùå Arquivo TG-263 n√£o encontrado: {args.tg_csv}")
    
    log(f"‚úÖ Arquivos de entrada OK: {args.data_json}, {args.tg_csv}")

    # -------------------------------------------------------------------------
    # 1) Leitura de dados e labels COM VALIDA√á√ÉO RIGOROSA
    # -------------------------------------------------------------------------
    log("\nüìã CARREGANDO DADOS...")
    dados = load_sanitized_json(args.data_json)
    tg_labels = load_label_set(args.tg_csv, "TG263-Primary Name")
    
    # VALIDA√á√ÉO CR√çTICA: Verificar qualidade dos dados carregados
    if len(dados) == 0:
        raise ValueError("‚ùå Nenhum exemplo v√°lido no arquivo JSON!")
    if len(tg_labels) == 0:
        raise ValueError("‚ùå Nenhuma label TG-263 v√°lida no CSV!")
    
    log(f"‚úÖ Exemplos carregados: {len(dados)} | Labels TG-263: {len(tg_labels)}")

    # VALIDA√á√ÉO: Compatibilidade entre JSON e CSV
    json_labels = set(str(d.get("tg263_name", "")).strip() for d in dados)
    json_labels.discard("")  # Remove vazios
    
    # Calcular overlap entre labels do JSON e do CSV
    overlap = json_labels.intersection(set(tg_labels))
    missing = json_labels - set(tg_labels)
    
    overlap_percent = 100 * len(overlap) / max(1, len(json_labels))
    log(f"üìä Compatibilidade JSON ‚Üî CSV: {len(overlap)}/{len(json_labels)} ({overlap_percent:.1f}%)")
    
    if overlap_percent < 50:
        log(f"‚ö†Ô∏è  AVISO: Baixa compatibilidade! Labels do JSON n√£o presentes no CSV:")
        for label in sorted(missing)[:10]:  # Mostrar at√© 10 exemplos
            log(f"    ‚Ä¢ '{label}'")
        log("üí° Isso pode indicar que o CSV est√° mal formatado ou colunas incorretas.")
    else:
        log(f"‚úÖ Boa compatibilidade entre JSON e CSV!")

    # Log de distribui√ß√£o (pr√©-remapeamento)
    dist_raw = Counter([str(d.get("tg263_name", "")).strip() for d in dados])
    log("\nüìà Top 10 labels no dataset de treino:")
    for name, cnt in dist_raw.most_common(10):
        log(f"  ‚Ä¢ {name}: {cnt}")

    # -------------------------------------------------------------------------
    # 2) Constru√ß√£o dos datasets (com remapeamento para whitelist TG)
    # -------------------------------------------------------------------------
    log("\nüî® CONSTRUINDO DATASETS...")
    train_ds, val_ds = build_datasets(dados, split_ratio=0.85, seed=args.seed, label_whitelist=tg_labels)
    
    # VALIDA√á√ÉO CR√çTICA: Datasets n√£o vazios
    if len(train_ds) == 0:
        raise ValueError("‚ùå Dataset de treino vazio ap√≥s processamento!")
    if len(val_ds) == 0:
        log("‚ö†Ô∏è  Dataset de valida√ß√£o vazio, usando parte do treino")
        # Usar √∫ltimos 10% do treino para valida√ß√£o
        split_point = int(0.9 * len(train_ds))
        val_data = {"text": train_ds["text"][split_point:]}
        train_data = {"text": train_ds["text"][:split_point]}
        train_ds = Dataset.from_dict(train_data)
        val_ds = Dataset.from_dict(val_data)
    
    log(f"‚úÖ Datasets: Treino={len(train_ds)}, Valida√ß√£o={len(val_ds)}")
    
    # Mostrar exemplo formatado
    if len(train_ds) > 0:
        example_text = train_ds[0]["text"]
        log(f"\nüìã Exemplo de treino formatado:")
        # CORRE√á√ÉO: evitar barra invertida em f-string
        display_text = example_text.replace('\n', '\\n')
        log(f"  {display_text}")

    # -------------------------------------------------------------------------
    # 3) Carregamento do modelo/tokenizer
    # -------------------------------------------------------------------------
    log("\nü§ñ CARREGANDO MODELO E TOKENIZER...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    log(f"üñ•Ô∏è  Device: {device}")
    
    if torch.cuda.is_available():
        try:
            gpu_name = torch.cuda.get_device_name()
            mem = torch.cuda.get_device_properties(0).total_memory
            log(f"üöÄ GPU: {gpu_name}")
            log(f"üíæ Mem√≥ria total: {human_bytes(mem)}")
            
            # Valida√ß√£o espec√≠fica para Tesla P40
            if "P40" in gpu_name or "Tesla" in gpu_name:
                log("‚ö†Ô∏è  GPU Tesla detectada - usando configura√ß√µes otimizadas para FP16 nativo")
                if args.batch > 4:
                    log(f"‚ö†Ô∏è  Batch size {args.batch} pode ser muito alto para P40, recomendado: ‚â§4")
        except Exception:
            log("‚ö†Ô∏è  N√£o foi poss√≠vel obter informa√ß√µes detalhadas da GPU")
    else:
        log("‚ö†Ô∏è  Executando em CPU - treino ser√° muito lento!")

    try:
        tokenizer = AutoTokenizer.from_pretrained(args.model_name, trust_remote_code=True)
        log(f"‚úÖ Tokenizer carregado: {args.model_name}")
    except Exception as e:
        raise RuntimeError(f"‚ùå Falha ao carregar tokenizer: {e}")

    # CORRE√á√ÉO DEFINITIVA: For√ßar FP32 para resolver grad_norm=inf
    try:
        log(f"üîÑ Carregando modelo: {args.model_name}...")
        # FOR√áAR FP32 para estabilidade num√©rica m√°xima
        model = AutoModelForCausalLM.from_pretrained(
            args.model_name,
            torch_dtype=torch.float32,  # OBRIGAT√ìRIO: FP32 para evitar overflow
            trust_remote_code=True,
            device_map=None,  # Carregamento manual para melhor controle
        )
        log(f"‚úÖ Modelo carregado em {model.dtype} (FP32 for√ßado para estabilidade)")
    except Exception as e:
        raise RuntimeError(f"‚ùå Falha ao carregar modelo: {e}")

    # Pad token (pode n√£o existir em alguns modelos)
    tokenizer, model = ensure_pad_token(tokenizer, model)

    # Melhorias de treino/mem√≥ria
    model.config.use_cache = False                # evita warnings e reduz mem√≥ria
    if torch.cuda.is_available():
        model = model.to(device)
        log(f"‚úÖ Modelo movido para {device}")

    # -------------------------------------------------------------------------
    # 4) LoRA: carregar adapters prontos ou preparar para treino
    # -------------------------------------------------------------------------
    log("\nüîß CONFIGURANDO LoRA...")
    if args.load_adapters:
        # Carrega adapters existentes sobre o backbone
        log(f"üìÇ Carregando adapters LoRA de: {args.load_adapters}")
        try:
            model = PeftModel.from_pretrained(model, args.load_adapters)
            # Em alguns ambientes, √© √∫til explicitar o adapter ativo:
            try:
                model.set_adapter(model.active_adapters)
            except Exception:
                pass
            log("‚úÖ Adapters LoRA carregados com sucesso")
        except Exception as e:
            raise RuntimeError(f"‚ùå Falha ao carregar adapters LoRA: {e}")
    elif not args.eval_only:
        # Preparar para treino adicionando cabe√ßalhos LoRA
        log("üî® Aplicando LoRA para treinamento...")
        model = attach_lora_for_training(
            model,
            target_modules=args.target_modules,
            r=args.lora_r,
            alpha=args.lora_alpha,
            dropout=args.lora_dropout,
        )
        
        # VALIDA√á√ÉO CR√çTICA: Garantir que h√° par√¢metros trein√°veis
        trainable_params = sum(1 for p in model.parameters() if p.requires_grad)
        if trainable_params == 0:
            raise RuntimeError("‚ùå ERRO CR√çTICO: Nenhum par√¢metro est√° marcado para treinamento!")
        log(f"‚úÖ {trainable_params} grupos de par√¢metros configurados para treinamento")

    # -------------------------------------------------------------------------
    # 5) Tokeniza√ß√£o + collator (m√°scara de loss s√≥ no 'Output:')
    # -------------------------------------------------------------------------
    log("\nüî§ TOKENIZANDO DATASETS...")
    def map_fn(batch):
        return tokenize_with_output_only_labels(batch, tokenizer, max_len=args.max_len)

    tokenized_train = train_ds.map(map_fn, batched=True, remove_columns=["text"])
    tokenized_val = val_ds.map(map_fn, batched=True, remove_columns=["text"])
    
    log(f"‚úÖ Tokeniza√ß√£o conclu√≠da: {len(tokenized_train)} treino, {len(tokenized_val)} valida√ß√£o")

    # =========================================================================
    # VALIDA√á√ïES CR√çTICAS P√ìS-TOKENIZA√á√ÉO
    # =========================================================================
    log("\nüîç EXECUTANDO VALIDA√á√ïES DE SANIDADE P√ìS-TOKENIZA√á√ÉO...")
    
    if len(tokenized_train) > 0:
        sample = tokenized_train[0]
        input_ids = sample['input_ids']
        labels = sample['labels']
        attention_mask = sample['attention_mask']
        
        # Contadores b√°sicos
        total_tokens = len(input_ids)
        masked_tokens = sum(1 for x in labels if x == -100)
        training_tokens = total_tokens - masked_tokens
        padding_tokens = sum(1 for x in attention_mask if x == 0)
        
        log(f"üìä AN√ÅLISE DO PRIMEIRO EXEMPLO TOKENIZADO:")
        log(f"  ‚Ä¢ Total de tokens: {total_tokens}")
        log(f"  ‚Ä¢ Tokens mascarados (-100): {masked_tokens}")
        log(f"  ‚Ä¢ Tokens para treino: {training_tokens}")
        log(f"  ‚Ä¢ Tokens de padding: {padding_tokens}")
        log(f"  ‚Ä¢ Propor√ß√£o treino/total: {100*training_tokens/max(1,total_tokens):.1f}%")
        
        # VALIDA√á√ïES CR√çTICAS
        if training_tokens == 0:
            raise RuntimeError("‚ùå ERRO CR√çTICO: Nenhum token est√° configurado para treinamento! "
                             "Problema na fun√ß√£o de mascaramento.")
        
        if training_tokens > total_tokens * 0.8:
            log(f"‚ö†Ô∏è  AVISO: Muitos tokens para treino ({training_tokens}/{total_tokens}). "
                 f"Pode indicar problema na detec√ß√£o do prefixo 'Output:'")
        
        if training_tokens < 5:
            log(f"‚ö†Ô∏è  AVISO: Poucos tokens para treino ({training_tokens}). "
                 f"Labels TG-263 podem estar muito curtas ou truncadas.")
        
        # Mostrar decodifica√ß√£o para verifica√ß√£o visual
        try:
            # Decodificar apenas partes relevantes para debug
            prefix_tokens = [id for id, label in zip(input_ids, labels) if label == -100][:10]
            training_tokens_sample = [id for id, label in zip(input_ids, labels) if label != -100][:10]
            
            prefix_text = tokenizer.decode(prefix_tokens, skip_special_tokens=True)
            training_text = tokenizer.decode(training_tokens_sample, skip_special_tokens=True)
            
            log(f"üìã VERIFICA√á√ÉO VISUAL:")
            log(f"  ‚Ä¢ Prefixo (mascarado): '{prefix_text}'")
            log(f"  ‚Ä¢ Tokens de treino: '{training_text}'")
            
            # Verificar se "Output:" est√° no prefixo mascarado
            if "Output" not in prefix_text and "Input" not in prefix_text:
                log(f"‚ö†Ô∏è  AVISO: Prefixo n√£o parece conter 'Input:' ou 'Output:'. "
                     f"Poss√≠vel problema na detec√ß√£o!")
            else:
                log(f"‚úÖ Prefixo parece estar correto (cont√©m Input/Output)")
                
        except Exception as e:
            log(f"‚ö†Ô∏è  N√£o foi poss√≠vel decodificar para verifica√ß√£o visual: {e}")
            
        log(f"‚úÖ Valida√ß√µes de sanidade aprovadas!")
    
    else:
        raise RuntimeError("‚ùå Dataset tokenizado de treino est√° vazio!")

    # Data collator personalizado que preserva gradientes
    class CustomDataCollator:
        def __init__(self, tokenizer):
            self.tokenizer = tokenizer
            
        def __call__(self, features):
            batch = {}
            
            # CORRE√á√ÉO: Usar tensor adequadamente
            batch["input_ids"] = torch.tensor([f["input_ids"] for f in features], dtype=torch.long)
            batch["attention_mask"] = torch.tensor([f["attention_mask"] for f in features], dtype=torch.long) 
            batch["labels"] = torch.tensor([f["labels"] for f in features], dtype=torch.long)
            
            return batch
    
    data_collator = CustomDataCollator(tokenizer)

    # -------------------------------------------------------------------------
    # 6) Treinamento (se n√£o for somente avalia√ß√£o)
    # -------------------------------------------------------------------------
    if not args.eval_only:
        log("\nüöÄ CONFIGURA√á√ÉO DE TREINAMENTO CORRIGIDA:")
        
        # CORRE√á√ÉO CR√çTICA: LR ultra-conservador para evitar overflow
        ultra_conservative_lr = min(args.lr, 1e-5)  # M√°ximo 1e-5 para estabilidade total
        if ultra_conservative_lr != args.lr:
            log(f"‚ö†Ô∏è  Learning rate ajustado para estabilidade m√°xima: {args.lr} ‚Üí {ultra_conservative_lr}")
        
        # CORRE√á√ÉO: Configura√ß√µes ultra-est√°veis para resolver grad_norm=inf
        args_train = TrainingArguments(
            output_dir=args.tmp_dir,
            per_device_train_batch_size=args.batch,
            per_device_eval_batch_size=max(1, args.batch // 2),  # Eval batch menor
            gradient_accumulation_steps=args.grad_accum,
            num_train_epochs=args.epochs,
            learning_rate=ultra_conservative_lr,  # CORRE√á√ÉO: LR ultra-conservador
            logging_steps=20,
            save_steps=100,
            eval_steps=100,
            eval_strategy="steps",            # CORRE√á√ÉO: usar eval_strategy (compatibilidade)
            save_total_limit=2,
            warmup_ratio=0.1,
            fp16=False,                       # CORRE√á√ÉO: modelo j√° em FP16, sem GradScaler
            bf16=False,                       # Tesla P40 n√£o suporta bf16
            dataloader_drop_last=True,
            remove_unused_columns=False,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            report_to=None,                   # Desabilitar wandb/tensorboard
            weight_decay=0.01,
            lr_scheduler_type="cosine",
            logging_dir="./logs",
            gradient_checkpointing=False,     # CORRE√á√ÉO: pode causar problemas com LoRA
            max_grad_norm=0.5,                # CORRE√á√ÉO: Clipping mais agressivo para evitar inf
            # ADICIONAL: Configura√ß√µes de estabilidade
            dataloader_num_workers=0,         # Evita problemas com multiprocessing
            save_safetensors=True,            # Formato moderno
            prediction_loss_only=True,       # S√≥ calcular loss na eval
            include_inputs_for_metrics=False,
            # Configura√ß√µes anti-OOM
            max_steps=-1,
            logging_first_step=True,
            save_only_model=True,
            # CR√çTICO para estabilidade num√©rica
            ddp_find_unused_parameters=False,
            dataloader_pin_memory=False,      # Pode causar OOM na P40
        )
        
        log(f"‚úÖ Configura√ß√µes de treino otimizadas:")
        log(f"  ‚Ä¢ Learning rate: {ultra_conservative_lr}")
        log(f"  ‚Ä¢ Batch size treino/eval: {args.batch}/{max(1, args.batch // 2)}")
        log(f"  ‚Ä¢ Gradient accumulation: {args.grad_accum}")
        log(f"  ‚Ä¢ FP16: {args_train.fp16} (modelo j√° est√° em FP32)")
        log(f"  ‚Ä¢ Max grad norm: {args_train.max_grad_norm}")

        # CALLBACK PERSONALIZADO: Monitor de estabilidade num√©rica
        from transformers import TrainerCallback
        
        class NaNMonitorCallback(TrainerCallback):
            """Monitora NaN/Inf na loss e para o treinamento se detectado"""
            
            def __init__(self):
                self.nan_count = 0
                self.max_nan_tolerance = 3  # M√°ximo de NaN consecutivos
                
            def on_log(self, args, state, control, model=None, logs=None, **kwargs):
                if logs is not None:
                    # Verificar train_loss
                    train_loss = logs.get("train_loss", None)
                    eval_loss = logs.get("eval_loss", None)
                    grad_norm = logs.get("grad_norm", None)
                    
                    # Detectar NaN/Inf
                    has_nan = False
                    if train_loss is not None and (not torch.isfinite(torch.tensor(train_loss))):
                        log(f"‚ö†Ô∏è  NaN/Inf detectado em train_loss: {train_loss}")
                        has_nan = True
                    if eval_loss is not None and (not torch.isfinite(torch.tensor(eval_loss))):
                        log(f"‚ö†Ô∏è  NaN/Inf detectado em eval_loss: {eval_loss}")
                        has_nan = True  
                    if grad_norm is not None and (not torch.isfinite(torch.tensor(grad_norm))):
                        log(f"‚ö†Ô∏è  NaN/Inf detectado em grad_norm: {grad_norm}")
                        has_nan = True
                    
                    if has_nan:
                        self.nan_count += 1
                        log(f"üí• INSTABILIDADE NUM√âRICA DETECTADA! Ocorr√™ncia #{self.nan_count}")
                        
                        if self.nan_count >= self.max_nan_tolerance:
                            log(f"‚ùå PARANDO TREINAMENTO: {self.nan_count} NaN/Inf consecutivos detectados!")
                            log(f"üí° Poss√≠veis solu√ß√µes:")
                            log(f"   ‚Ä¢ Reduzir learning rate (atual: {args.learning_rate})")
                            log(f"   ‚Ä¢ Reduzir batch size (atual: {args.per_device_train_batch_size})")
                            log(f"   ‚Ä¢ Usar FP32 em vez de FP16")
                            log(f"   ‚Ä¢ Aumentar gradient clipping (atual: {args.max_grad_norm})")
                            control.should_training_stop = True
                    else:
                        # Reset contador se n√£o houver NaN
                        self.nan_count = 0
                        
                    # Log normal de progresso
                    if state.global_step % 20 == 0:  # A cada 20 steps
                        progress = 100 * state.global_step / max(1, state.max_steps) if state.max_steps > 0 else 0
                        train_loss_str = f"{train_loss:.4f}" if train_loss is not None else "N/A"
                        grad_norm_str = f"{grad_norm:.4f}" if grad_norm is not None else "N/A"
                        log(f"üìä Step {state.global_step}: loss={train_loss_str}, "
                            f"grad_norm={grad_norm_str}, progresso={progress:.1f}%")

        nan_monitor = NaNMonitorCallback()

        trainer = Trainer(
            model=model,
            args=args_train,
            train_dataset=tokenized_train,
            eval_dataset=tokenized_val,
            processing_class=tokenizer,       # CORRE√á√ÉO: usar processing_class= (vers√£o mais nova)
            data_collator=data_collator,
            callbacks=[
                EarlyStoppingCallback(early_stopping_patience=3),
                nan_monitor,  # NOVO: Monitor de estabilidade num√©rica
            ],
        )

        log("\n" + "=" * 70)
        log("INICIANDO TREINAMENTO")
        log("=" * 70)
        
        # DIAGN√ìSTICO: Verificar par√¢metros trein√°veis antes do treinamento
        trainable_params = []
        for name, param in model.named_parameters():
            if param.requires_grad:
                trainable_params.append((name, param.dtype, param.requires_grad))
        
        log(f"Par√¢metros trein√°veis encontrados: {len(trainable_params)}")
        if len(trainable_params) == 0:
            log("‚ùå ERRO: Nenhum par√¢metro est√° configurado para treinar!")
            return
            
        # Mostrar alguns exemplos
        for name, dtype, req_grad in trainable_params[:5]:
            log(f"  {name}: dtype={dtype}, requires_grad={req_grad}")
        
        trainer.train()
        log("‚úì Treinamento conclu√≠do!")

        # ---------------------------------------------------------------------
        # 6.1) Salvamento (adapters / merged / both)
        # ---------------------------------------------------------------------
        saved_paths = save_model_variants(model, tokenizer, args.out_dir, args.save_mode)

        # ---------------------------------------------------------------------
        # 6.2) (Opcional) Upload para o Hugging Face Hub
        # ---------------------------------------------------------------------
        if args.push_to_hub:
            if not _HF_AVAILABLE:
                log("‚ùå huggingface_hub n√£o est√° instalado. Fa√ßa: pip install huggingface_hub")
            elif not args.hub_repo_id:
                log("‚ùå --push_to_hub requer --hub_repo_id (ex.: 'seu-usuario/seu-modelo').")
            else:
                try:
                    for path in saved_paths:
                        repo_id = args.hub_repo_id
                        # Se 'both', crie sufixos diferentes automaticamente
                        if len(saved_paths) > 1:
                            suffix = os.path.basename(path.rstrip("/"))
                            repo_id = f"{args.hub_repo_id}-{suffix}"
                        log(f"üÜô Criando/atualizando repo: {repo_id}")
                        create_repo(repo_id, exist_ok=True)
                        upload_folder(repo_id=repo_id, folder_path=path, commit_message="Upload model")
                        log(f"‚úì Upload conclu√≠do em: {repo_id}")
                except Exception as e:
                    log(f"‚ùå Falha no upload para o Hub: {e}")

    else:
        log("\nModo de avalia√ß√£o apenas ‚Äî pulando treinamento/salvamento.")

    # -------------------------------------------------------------------------
    # 7) Avalia√ß√£o com constrained decoding (greedy)
    # -------------------------------------------------------------------------
    log("\nConstruindo trie de labels TG-263 para avalia√ß√£o...")
    trie = build_label_trie(tokenizer, tg_labels)

    # Reconstr√≥i pares a partir do texto formatado (val_ds)
    eval_pairs: List[Tuple[str, str]] = []
    for t in val_ds["text"]:
        try:
            x = t.split("Input: ", 1)[1].split("\nOutput:", 1)[0].strip()
            y = t.split("Output:", 1)[1].strip().split("\n")[0].strip()
            eval_pairs.append((x, y))
        except Exception:
            continue

    # Determina qual objeto de modelo usar para infer√™ncia:
    # - Se salvamos 'merged' agora, o `model` pode ter virado merged internamente.
    # - No geral, `model` est√° pronto para infer√™ncia.
    inference_model = model

    acc, errors = evaluate_on_set(
        inference_model,
        tokenizer,
        trie,
        eval_pairs,
        tg_labels,
        device,
        verbose=args.eval_verbose,
        min_avg_logprob=args.min_avg_logprob,
        max_new_tokens=args.max_new_tokens,
    )
    log(f"Acur√°cia Top-1 (Val, ap√≥s normaliza√ß√£o): {acc:.2%}")

    if errors[:10]:
        log("Erros (amostra at√© 10):")
        for e in errors[:10]:
            x, y_true, y_pred, meta = e
            log(f"  '{x}' ‚Üí {y_true} | pred={y_pred}  meta={meta}")

    log("\n" + "=" * 70)
    log("FINE-TUNING TG-263 CONCLU√çDO")
    log("=" * 70)
    log(f"Labels TG263 usados: {len(tg_labels)} (coluna 'TG263-Primary Name')")
    log("Para infer√™ncia em produ√ß√£o: use translate_strict(model, tokenizer, trie, 'texto', device).")


if __name__ == "__main__":
    main()
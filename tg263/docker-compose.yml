services:
  vllm:
    image: rtmedical/anatomind-engine:Llama-8B
    ports:
      - "8000:8000"
    environment:
      - HF_HUB_OFFLINE=1
      - VLLM_NO_USAGE_STATS=1
    command: >
      --model /opt/model
      --served-model-name tg263-8b 
      --max-model-len 8192 
      --tensor-parallel-size 1 
      --port 8000 
      --host 0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  tgi:
    image: rtmedical/anatomind-engine:Qwen2.5-7B-TGI

    ports:
      - "8001:80"
    environment:
      - HF_HUB_OFFLINE=1
      - TRANSFORMERS_OFFLINE=1
      - TGI_DISABLE_TELEMETRY=1
      - CUDA_VISIBLE_DEVICES=0
      - TORCH_CUDA_ARCH_LIST=6.1
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128
      - CUDA_LAUNCH_BLOCKING=1
      - FLASH_ATTENTION_FORCE_DISABLE=1
      - FLASHINFER_FORCE_DISABLE=1
      - DISABLE_FLASHINFER=1
      - DISABLE_FLASH_ATTENTION=1
      - TORCH_BACKENDS_CUDA_ENABLE_FLASH_ATTENTION=0
      - ATTN_IMPLEMENTATION=eager
    command: >
      --model-id /opt/model 
      --disable-custom-kernels
      --port 80
      --hostname 0.0.0.0
      --dtype float16
      --quantize bitsandbytes-nf4
      --max-batch-total-tokens 512
      --max-total-tokens 512
      --max-input-length 256
      --max-batch-prefill-tokens 256
      --max-batch-size 1
      --cuda-memory-fraction 0.3
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    volumes:
      - ./labels:/app/labels:ro
      - ./config:/app/config:ro
    environment:
      - PROVIDER=${PROVIDER:-vllm}
      - VLLM_URL=http://vllm:8000
      - TGI_URL=http://tgi:80
      - DEFAULT_CHAT_MODEL=tg263-8b
      - TG263_CSV=/app/labels/TG263.csv
      - ABSTAIN_SIM_THRESHOLD=${ABSTAIN_SIM_THRESHOLD:-0.85}
      - MAX_LABELS=${MAX_LABELS:-5}
    depends_on:
      vllm:
        condition: service_healthy
      #tgi:
      #  condition: service_healthy
    healthcheck:
      test: ["CMD", "/app/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
